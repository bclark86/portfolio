{
  "hash": "8c0737acc25954306c28413f4a0101ea",
  "result": {
    "markdown": "---\ntitle: Quantifying the Risk of Experimentation Results\nauthor: Bryan Clark\ndate: '2023-01-30'\ncategories:\n  - code\n  - stats\nimage: boostrapping_visual.png\nbibliography: references.bib\nformat:\n  html:\n    code-fold: false\n    highlight-style: atom-one\n---\n\nTL;DR: Using bootstrapping to quantify risk. \n\n## Introduction\n\nThis post outlines an approach of using bootstrapping to quantify risk associated with the results of an A/B test. I've encountered this scenario during my career and found this approach to help reframe the conversation from _\"was it significant?\"_ towards _\"what is the risk?\"_\n\n## Problem Overview\n\n##### Scenario\n\nLet's assume you work for a company that just ran an A/B test on a conversion funnel where each conversion from a visitor can have a different payoff. This could be an e-commerce website where each conversion is a variable purchase amount or subscription funnel that has different plan options. The goal of the test was either to not do any harm (or ideally improve revenue). The A/B test consisted of two arms: `control` and `test`. \n\nThe results of the experiment are directionally positive, but not statistically significant based on our predetermined p-value cutoff. Our experiment shows that there is no statistically significant difference between `control` and `test`, so we've accomplished our goal of doing no harm and can move forward, right? Not so fast. Just because the result was not statistically significant does NOT mean there is no risk with the `test` variation. \n\nThe goal of our analysis is to quantify the risk of implementation as it pertains to ARPV. \n\n##### Defining Uncertainty and Risk\n\nUncertainty and risk can mean different things to different people, so we'll adopt the definitions used in _How to Measure Anything_ [@hubbard2014]:\n\n- __Uncertainty:__ The lack of complete certaintym that is, the existence of more than one possibility. The \"true\" outcome/state/result/value is not known. \n- __Measurement of Uncertainty:__ A set of probabilities assigned to a set of possibilities. For example: \"There is a 60% chance this market will more than double in five years, a 30% chance this market will grow at a slower rate, and a 10% chance the market will shrink in the same period.\"\n- __Risk:__ A state of uncertainty where some of the possibilities involve a loss, catastrophe, or other undesirable outcome. \n- __Measurement of Risk:__ A set of possibilities, each with quantified probabilities and quantified losses. For example: \"We believe there is a 40% chance the proposed oil well will be dry with a loss of $12 million in exploratory drilling costs.\"\n\n##### Goal\n\nQuantify the uncertainty and risk associated with the results of the \"do no harm\" A/B test.  \n\n## Analytical Approach\n\n##### Data Generation\n\nFirst, let's generate some fake test results to use for our example. The specific distributions don't matter as much for this, but we'll be sampling a set of purchases from a binomial distribution and purchase payoffs from a lognormal distribution. Combined, we get order values that consist of 0s and purchases with variable payoffs. \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom numpy.random import default_rng\n\n# random number generator\nrng = default_rng(934)\n\n# data generator\ndef generate_data(n, p, mu, sigma):\n    # generate conversions from sample size n with probability p\n    conversions = rng.binomial(n, p)\n    # for each conversion, generate a payoff\n    payoffs = rng.lognormal(mu, sigma, conversions).round(2)\n    # generate a payoff of 0 for all non-conversions\n    non_conversions = np.zeros((n - payoffs.size))\n    # combine into a single array of results\n    results = np.concatenate([payoffs, non_conversions])\n    return results\n\n# control parameters\ncontrol_params = {\n  'n': 15_000,\n  'p': 0.0375,\n  'mu': 3,\n  'sigma': 1\n}\n\n# test parameters\ntest_params = {\n  'n': 15_000,\n  'p': 0.0365,\n  'mu': 3.1,\n  'sigma': 1\n}\n\ncontrol = generate_data(**control_params)\ntest = generate_data(**test_params)\n\nprint(\n  f'Control ARPV: ${control.mean():.2f}; Test ARPV: ${test.mean():.2f}'\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nControl ARPV: $1.12; Test ARPV: $1.13\n```\n:::\n:::\n\n\n##### Significance Testing\n\nWhen we run a t-test on our experiment results, we see a directionally positive increase in ARPV and a p-value that suggests no detectable difference in ARPV between our `test` and `control` variants. \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom scipy.stats import ttest_ind\n\nstat, pval = ttest_ind(control, test, equal_var=False)\n\nprint(f'ARPV Lift: {test.mean() / control.mean() - 1:.0%}')\nprint(f'P-value: {pval:.4f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nARPV Lift: 1%\nP-value: 0.8930\n```\n:::\n:::\n\n\n##### Quantifying Uncertainty and Risk\n\nHowever, just because the p-value is insignificant, it does not mean that there is no risk associated with the `test` variant. \n\nTo help us better understand the uncertainty and risk, we can leverage nonparametric bootstrapping [@pennstatebootstrapping] to estimate the sampling distribution of the difference between the `test` and `control` variants. Using this distribution, we can then calculate our measures of uncertainty and risk.\n\n![Bootstrapping](boostrapping_visual.png)\n\nWe'll get there in three steps:\n\n1. Generate bootstrap distribution for `test` and `control` variants\n2. Estimate the distribution for the difference of `test -  control`\n3. Calculate uncertainty and risk measurements\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\n\ndef bootstrap_distribution(arr, n_boots=10_000):\n    return np.array([\n        rng.choice(arr, size=arr.size, replace=True).mean()\n        for _ in range(n_boots)\n    ])\n\n# control bootstrap distribution\nboot_control = bootstrap_distribution(control)\n# test bootstrap distribution\nboot_test = bootstrap_distribution(test)\n# difference \nboot_diff = boot_test - boot_control\n\n# combine for plotting\nboot_df = pd.concat([\n    pd.DataFrame(\n      boot_control, columns=['value']\n    ).assign(name='control'),\n    pd.DataFrame(\n      boot_test, columns=['value']\n    ).assign(name='test'),\n    pd.DataFrame(\n      boot_diff, columns=['value']\n    ).assign(name='difference')\n])\n\nboot_df['group'] = np.where(\n  boot_df['name'] != 'difference',\n  'variant', 'comparison'\n)\n```\n:::\n\n\nLooking at the distributions of each variant below, there is quite a bit of overlap, which mimics what we would expect given the small difference in lift and the high p-value in our significance testing. \n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport plotnine as p9\n(\n    p9.ggplot(\n      boot_df.query('group != \"comparison\"'),\n      p9.aes(x='value', fill='name'))\n    + p9.geom_histogram(\n      bins=50, alpha=0.5, position='identity'\n    )\n    + p9.scale_fill_manual(values=[\"#999999\", \"#009E73\"])\n    + p9.labs(\n      title='Average Revenue Per Visitor (ARPV) Bootstrap Distributions',\n      fill='Variant',\n      x='ARPV',\n      y='Trials'\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n<ggplot: (8771783057120)>\n```\n:::\n:::\n\n\nHowever, when we look at the distribution of the difference, we can see a fair amount of the distribution sitting below 0 (referenced by the dotted line).\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n(\n    p9.ggplot(\n      boot_df.query('group == \"comparison\"'),\n      p9.aes(x='value')\n    )\n    + p9.geom_histogram(\n      bins=50, \n      alpha=0.5, \n      position='identity',\n      fill='#0072B2'\n    )\n    + p9.geom_vline(xintercept=0, linetype='dashed')\n    + p9.labs(\n      title='Average Revenue Per Visitor (ARPV) Difference Distribution',\n      fill='',\n      x='ARPV Difference',\n      y='Trials'\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n<ggplot: (8771783756329)>\n```\n:::\n:::\n\n\nLastly, to calculate uncertainty and risk, we'll estimate the percentage of the distribution that is below 0 as well as the average loss associated with a suboptimal outcome. \n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nrisk_prob = np.mean([boot_diff < 0])\navg_risk = np.median(boot_diff[boot_diff < 0])\nvar_10 = np.percentile(boot_diff, q=10)\n\nsummary_str = f\"\"\"\nWe believe there is a {risk_prob:.0%} chance the test variant will reduce ARPV, with an average loss of ${avg_risk:.2f} in ARPV. \n\nWe estimate a 10% chance that the loss will exceed ${var_10:.2f} per visitor.\n\"\"\"\n\nprint(summary_str)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nWe believe there is a 44% chance the test variant will reduce ARPV, with an average loss of $-0.07 in ARPV. \n\nWe estimate a 10% chance that the loss will exceed $-0.13 per visitor.\n\n```\n:::\n:::\n\n\n## Outcome\n\nAs a result of our analysis, we can have a much more nuanced conversation with business stakeholders about the uncertainty and risk of implementing an intervention that, on the surface, yielded directionally positive results and no statistically significant difference from our control experience. Perhaps this amount of risk is not tolerable for the business and we can move the discussion toward the ROI of reducing uncertainty by collecting more data via another experiment. This is not a conversation that would have taken place if we simply accepted the results of the significance test. \n\nWhile alternatives such as proper initial experiment design that accounted for acceptable risk tolerance or bayesian analysis methods could also be useful, this is an example of how bootstrapping can be a useful tool for extracting more information from experiment data. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}