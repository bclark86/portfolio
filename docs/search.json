[
  {
    "objectID": "posts/experiment-risk/index.html",
    "href": "posts/experiment-risk/index.html",
    "title": "Quantifying the Risk of Experimentation Results",
    "section": "",
    "text": "TL;DR: Using bootstrapping to quantify risk."
  },
  {
    "objectID": "posts/experiment-risk/index.html#introduction",
    "href": "posts/experiment-risk/index.html#introduction",
    "title": "Quantifying the Risk of Experimentation Results",
    "section": "Introduction",
    "text": "Introduction\nThis post outlines an approach of using bootstrapping to quantify risk associated with the results of an A/B test. I’ve encountered this scenario during my career and found this approach to help reframe the conversation from “was it significant?” towards “what is the risk?”"
  },
  {
    "objectID": "posts/experiment-risk/index.html#problem-overview",
    "href": "posts/experiment-risk/index.html#problem-overview",
    "title": "Quantifying the Risk of Experimentation Results",
    "section": "Problem Overview",
    "text": "Problem Overview\n\nScenario\nLet’s assume you work for a company that just ran an A/B test on a conversion funnel where each conversion from a visitor can have a different payoff. This could be an e-commerce website where each conversion is a variable purchase amount or subscription funnel that has different plan options. The goal of the test was either to not do any harm (or ideally improve revenue). The A/B test consisted of two arms: control and test.\nThe results of the experiment are directionally positive, but not statistically significant based on our predetermined p-value cutoff. Our experiment shows that there is no statistically significant difference between control and test, so we’ve accomplished our goal of doing no harm and can move forward, right? Not so fast. Just because the result was not statistically significant does NOT mean there is no risk with the test variation.\nThe goal of our analysis is to quantify the risk of implementation as it pertains to ARPV.\n\n\nDefining Uncertainty and Risk\nUncertainty and risk can mean different things to different people, so we’ll adopt the definitions used in How to Measure Anything (Hubbard 2014):\n\nUncertainty: The lack of complete certaintym that is, the existence of more than one possibility. The “true” outcome/state/result/value is not known.\nMeasurement of Uncertainty: A set of probabilities assigned to a set of possibilities. For example: “There is a 60% chance this market will more than double in five years, a 30% chance this market will grow at a slower rate, and a 10% chance the market will shrink in the same period.”\nRisk: A state of uncertainty where some of the possibilities involve a loss, catastrophe, or other undesirable outcome.\nMeasurement of Risk: A set of possibilities, each with quantified probabilities and quantified losses. For example: “We believe there is a 40% chance the proposed oil well will be dry with a loss of $12 million in exploratory drilling costs.”\n\n\n\nGoal\nQuantify the uncertainty and risk associated with the results of the “do no harm” A/B test."
  },
  {
    "objectID": "posts/experiment-risk/index.html#analytical-approach",
    "href": "posts/experiment-risk/index.html#analytical-approach",
    "title": "Quantifying the Risk of Experimentation Results",
    "section": "Analytical Approach",
    "text": "Analytical Approach\n\nData Generation\nFirst, let’s generate some fake test results to use for our example. The specific distributions don’t matter as much for this, but we’ll be sampling a set of purchases from a binomial distribution and purchase payoffs from a lognormal distribution. Combined, we get order values that consist of 0s and purchases with variable payoffs.\n\nimport numpy as np\nfrom numpy.random import default_rng\n\n# random number generator\nrng = default_rng(934)\n\n# data generator\ndef generate_data(n, p, mu, sigma):\n    # generate conversions from sample size n with probability p\n    conversions = rng.binomial(n, p)\n    # for each conversion, generate a payoff\n    payoffs = rng.lognormal(mu, sigma, conversions).round(2)\n    # generate a payoff of 0 for all non-conversions\n    non_conversions = np.zeros((n - payoffs.size))\n    # combine into a single array of results\n    results = np.concatenate([payoffs, non_conversions])\n    return results\n\n# control parameters\ncontrol_params = {\n  'n': 15_000,\n  'p': 0.0375,\n  'mu': 3,\n  'sigma': 1\n}\n\n# test parameters\ntest_params = {\n  'n': 15_000,\n  'p': 0.0365,\n  'mu': 3.1,\n  'sigma': 1\n}\n\ncontrol = generate_data(**control_params)\ntest = generate_data(**test_params)\n\nprint(\n  f'Control ARPV: ${control.mean():.2f}; Test ARPV: ${test.mean():.2f}'\n)\n\nControl ARPV: $1.12; Test ARPV: $1.13\n\n\n\n\nSignificance Testing\nWhen we run a t-test on our experiment results, we see a directionally positive increase in ARPV and a p-value that suggests no detectable difference in ARPV between our test and control variants.\n\nfrom scipy.stats import ttest_ind\n\nstat, pval = ttest_ind(control, test, equal_var=False)\n\nprint(f'ARPV Lift: {test.mean() / control.mean() - 1:.0%}')\nprint(f'P-value: {pval:.4f}')\n\nARPV Lift: 1%\nP-value: 0.8930\n\n\n\n\nQuantifying Uncertainty and Risk\nHowever, just because the p-value is insignificant, it does not mean that there is no risk associated with the test variant.\nTo help us better understand the uncertainty and risk, we can leverage nonparametric bootstrapping (Penn State 2018) to estimate the sampling distribution of the difference between the test and control variants. Using this distribution, we can then calculate our measures of uncertainty and risk.\n\n\n\nBootstrapping\n\n\nWe’ll get there in three steps:\n\nGenerate bootstrap distribution for test and control variants\nEstimate the distribution for the difference of test -  control\nCalculate uncertainty and risk measurements\n\n\nimport pandas as pd\n\ndef bootstrap_distribution(arr, n_boots=10_000):\n    return np.array([\n        rng.choice(arr, size=arr.size, replace=True).mean()\n        for _ in range(n_boots)\n    ])\n\n# control bootstrap distribution\nboot_control = bootstrap_distribution(control)\n# test bootstrap distribution\nboot_test = bootstrap_distribution(test)\n# difference \nboot_diff = boot_test - boot_control\n\n# combine for plotting\nboot_df = pd.concat([\n    pd.DataFrame(\n      boot_control, columns=['value']\n    ).assign(name='control'),\n    pd.DataFrame(\n      boot_test, columns=['value']\n    ).assign(name='test'),\n    pd.DataFrame(\n      boot_diff, columns=['value']\n    ).assign(name='difference')\n])\n\nboot_df['group'] = np.where(\n  boot_df['name'] != 'difference',\n  'variant', 'comparison'\n)\n\nLooking at the distributions of each variant below, there is quite a bit of overlap, which mimics what we would expect given the small difference in lift and the high p-value in our significance testing.\n\nimport plotnine as p9\n(\n    p9.ggplot(\n      boot_df.query('group != \"comparison\"'),\n      p9.aes(x='value', fill='name'))\n    + p9.geom_histogram(\n      bins=50, alpha=0.5, position='identity'\n    )\n    + p9.scale_fill_manual(values=[\"#999999\", \"#009E73\"])\n    + p9.labs(\n      title='Average Revenue Per Visitor (ARPV) Bootstrap Distributions',\n      fill='Variant',\n      x='ARPV',\n      y='Trials'\n    )\n)\n\n\n\n\n<ggplot: (8771783057120)>\n\n\nHowever, when we look at the distribution of the difference, we can see a fair amount of the distribution sitting below 0 (referenced by the dotted line).\n\n(\n    p9.ggplot(\n      boot_df.query('group == \"comparison\"'),\n      p9.aes(x='value')\n    )\n    + p9.geom_histogram(\n      bins=50, \n      alpha=0.5, \n      position='identity',\n      fill='#0072B2'\n    )\n    + p9.geom_vline(xintercept=0, linetype='dashed')\n    + p9.labs(\n      title='Average Revenue Per Visitor (ARPV) Difference Distribution',\n      fill='',\n      x='ARPV Difference',\n      y='Trials'\n    )\n)\n\n\n\n\n<ggplot: (8771783756329)>\n\n\nLastly, to calculate uncertainty and risk, we’ll estimate the percentage of the distribution that is below 0 as well as the average loss associated with a suboptimal outcome.\n\nrisk_prob = np.mean([boot_diff < 0])\navg_risk = np.median(boot_diff[boot_diff < 0])\nvar_10 = np.percentile(boot_diff, q=10)\n\nsummary_str = f\"\"\"\nWe believe there is a {risk_prob:.0%} chance the test variant will reduce ARPV, with an average loss of ${avg_risk:.2f} in ARPV. \n\nWe estimate a 10% chance that the loss will exceed ${var_10:.2f} per visitor.\n\"\"\"\n\nprint(summary_str)\n\n\nWe believe there is a 44% chance the test variant will reduce ARPV, with an average loss of $-0.07 in ARPV. \n\nWe estimate a 10% chance that the loss will exceed $-0.13 per visitor."
  },
  {
    "objectID": "posts/experiment-risk/index.html#outcome",
    "href": "posts/experiment-risk/index.html#outcome",
    "title": "Quantifying the Risk of Experimentation Results",
    "section": "Outcome",
    "text": "Outcome\nAs a result of our analysis, we can have a much more nuanced conversation with business stakeholders about the uncertainty and risk of implementing an intervention that, on the surface, yielded directionally positive results and no statistically significant difference from our control experience. Perhaps this amount of risk is not tolerable for the business and we can move the discussion toward the ROI of reducing uncertainty by collecting more data via another experiment. This is not a conversation that would have taken place if we simply accepted the results of the significance test.\nWhile alternatives such as proper initial experiment design that accounted for acceptable risk tolerance or bayesian analysis methods could also be useful, this is an example of how bootstrapping can be a useful tool for extracting more information from experiment data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Quantifying the Risk of Experimentation Results\n\n\n\n\n\n\n\ncode\n\n\nstats\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nBryan Clark\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I have an insatiable curiosity and appetite for learning. I love solving problems that involve blending an understanding of people and process with quantitative data analysis.\n\nBACKGROUND\nI’m experienced in generating value with data / data science (segmentation, analytics, modeling, experimentation, machine learning) for real estate, subscription / e-commerce, product development, monetization, acquisition, growth, and CRM teams within retail (Petco; H&M) and B2C SaaS companies (Dashlane; Noom).\nI have two master’s degrees:\n\nMBA in Strategy / Tech Product Management from NYU Stern (2022)\nMS in Applied Business Analytics from Boston University (2018)\n\n\n\nSTRENGTHS\nPattern Recognition. Data-Driven Decision-Making. Synthesis. Resourcefulness. Adaptability.\nTop 5 CliftonStrengths:\n\nFuturistic: I am inspired by the future and what could be. I energize others with visions of the future.\nLearner: great desire to learn and continuously improve. The process of learning, rather than the outcome, excites me.\nCommand: I have presence. I can take control of a situation and make decisions.\nRestorative: I am adept at dealing with problems. I am good at figuring out what is wrong and resolving it.\nIdeation: I am fascinated by ideas. I am able to find connections between seemingly disparate phenomena.\n\n\n\nFOR FUN\nReading. Learning. Health and wellness. Watching sports. Playing hockey. Learning to golf."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Quantifying the Risk of Experimentation Results\n\n\n\n\n\n\n\ncode\n\n\nstats\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nBryan Clark\n\n\n\n\n\n\nNo matching items"
  }
]